{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check List \n",
    "\n",
    "@22.2 # intro abstract  TODO: Add references \n",
    "\n",
    "@22.2 # ML  thers and future                     DONE\n",
    "\n",
    "@22.2 # math behind  attakcs                     DONE\n",
    "\n",
    "@22.2# intutive attacks  2    TODO Add exmaples \n",
    "\n",
    "\n",
    "# LLm attacking by model 1  (code)\n",
    "\n",
    "# our approuch \n",
    "\n",
    "# setup attack model, and LLm (code)\n",
    "\n",
    "# attack model (code)\n",
    "\n",
    "# training model (code)\n",
    "\n",
    "# hard coded logic prompt( code)\n",
    "\n",
    "# fintuning by masking (code)\n",
    "\n",
    "# loop \n",
    "\n",
    "# blocking defance is not defence \n",
    "\n",
    "# defances \n",
    "\n",
    "# attacks inruative\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Title:** Parasite Modeling: Unveiling New Vulnerabilities in Large Language Model Applications\n",
    "\n",
    "**Abstract:**\n",
    "In this paper, we delve into the burgeoning domain of Large Language Models (LLMs) and their integration across diverse applications such as computer vision, recommendation systems, and decision-making tools. Our exploration pivots around a novel concept: the \"Parasite Model\" attack (PS). This approach deviates from traditional direct and indirect injunction methods, aiming to exploit the unique vulnerabilities inherent in LLM-integrated applications. By transcending the conventional understanding of data and instruction dichotomy, we unveil a spectrum of security risks including data theft, worming, misinformation, and injunctive attacks, especially in contexts where LLMs converge with other technologies like computer vision. Our focus is to analyze how these models, when prompted by external or inter-model interactions, can become susceptible to various attack vectors, thus questioning the security of data in pre-prompting scenarios.\n",
    "\n",
    "We present a comprehensive taxonomy from a computer security perspective, offering a systematic investigation into these impacts and vulnerabilities. Through empirical studies on real-world systems, Gemma-7(Google), Bing(Microsoft), GPT-4(OpenAI) powered Chat and code-completion engines, as well as synthetic applications built on GPT-4, we demonstrate the practical viability of our attacks. We highlight how the processing of observed prompts and outputs by Parasite models can lead to robust mechanisms for injection attacks. \n",
    "\n",
    "However, effective countermeasures against these emerging threats remain scarce. We propose an innovative for new approach direction, utilizing the same observational techniques employed in PS attacks, to develop an encoder-decoder model that enhances security. This model challenges the current paradigm of protection by hard-coded blocking by toxicity threshold score that is given by another classification model after analysis of the output, which, while limiting LLM capabilities, fails to offer long-term solutions. By understanding and acknowledging these vulnerabilities, and providing key insights into their implications, we aim to pave the way for a new era of AI security. This will not only enhance the robustness of LLM applications but also foster a growing market for AI security solutions, reminiscent of the early internet era's rapid evolution in cybersecurity awareness and technologies.\n",
    "\n",
    "**Introduction:**\n",
    "Introduction:\n",
    "The integration of Large Language Models (LLMs) into various sectors represents a significant milestone in the evolution of artificial intelligence. These models, celebrated for their versatility and adaptability, have revolutionized fields as diverse as computer vision and recommendation systems. However, with great power comes great responsibility. The increasing reliance on LLMs has unveiled numerous security vulnerabilities, particularly in terms of manipulation potential. This paper introduces a groundbreaking perspective on these vulnerabilities,PS method observes and exploits unique vulnerabilities, learning directly from the LLM itself. Our approach treats the LLM as a black box, not attempting to decipher its internal mechanisms but rather reverse engineering the LLM's \"mind.\" This process allows us to project the space of the current task, such as incorporating toxicity as an additional prompt. Hence, we have termed this method the PS.\n",
    "\n",
    "In this context, the LLM becomes the host for the attack model, up until the Parasite Model is sufficiently developed to take control of its LLM host. This enables it to initiate actions or outputs that are not hard-coded blocks. The Parasite Model attack shows promise as a universal attack method, easily adaptable to most well-developed LLMs and applicable to a variety of tasks. This adaptability stems from the fact that most LLM tasks share similar spaces. Additionally, a well-pre-trained Parasite model can be fine-tuned to shift to new tasks. We will demonstrate how the real injection rules of PS models are learned during pre-training, understanding the relationship of how combinations of words can shift inputs within the task spaces. Finally, fine-tuning adds context manipulation skills to this specific species of injective attack.\n",
    "\n",
    "Traditionally, the interaction between users and LLMs has been straightforward – users prompt, and models respond. However, this dynamic changes when inputs are not directly user-generated but come from external or inter-model sources. Such scenarios blur the lines between data and instructions, creating a fertile ground for security breaches. We argue that in the absence of robust security measures, no data can be deemed secure against 'injunctive attacks' – a method where harmful instructions are disguised as benign prompts.\n",
    "\n",
    "Our research presents a detailed taxonomy from a computer security perspective, systematically examining the various impacts and vulnerabilities LLMs face. We explore how these models, when integrated with other systems like computer vision, become vulnerable to a range of attacks including data theft, worming, the spread of false information, and scanning internet attacks. The core of our investigation revolves around the \"Parasite Model\" approach, a sophisticated attack strategy that differs significantly from conventional methods. \n",
    "\n",
    "As we conclude, we highlight the current inadequacy of mitigation strategies against these novel threats. Our proposed an diractions for new encoder-decoder security filters models, derived from the observational tactics used in Parasite Model attacks, offers a new line of defense. This model challenges the existing block protection methods, which, while limiting the capabilities of LLMs, do not present a viable long-term solution. By shedding light on these vulnerabilities and proposing innovative solutions, our research aims to spearhead the development of effective AI security measures, fostering a safer and more robust environment for the utilization of LLMs in various applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Motivation:**\n",
    "\n",
    "The oft-quoted phrase, \"AI will replace us in our jobs,\" resonates as a common apprehension among many. Yet, in terms of security, we are still far from entrusting basic intuitive tasks to AI models. Operations not explicitly defined in a model's programming, particularly those influenced by external sources such as user inputs or data collection, open a substantial window to security threats. It would be unfortunate if avoidance were our only recourse. Consider a parallel with the early days of the internet: rather than shunning its use due to security risks, we chose to evolve and enhance cybersecurity measures. This approach aligns with our vision for AI – not merely to restrict or block certain actions or inputs, but to intelligently navigate and correct them in line with trained policies and ethical standards.\n",
    "\n",
    "The make-sense solutions aims to narrow the security gap in AI applications while minimizing the impact on model performance. By analyzing and adjusting outputs and actions to conform to predefined behavioral policies, we can glean insights that enable ongoing model improvements and adaptations.\n",
    "\n",
    "To contextualize, let's consider a brief example of real-world security risks that could arise from granting AI models decision-making authority. Our focus is not on dystopian scenarios often depicted in media, where AI gains control over the world. Instead, we concentrate on tangible, immediate security challenges that could emerge from the evolving capabilities of AI systems. By understanding these risks[https://arxiv.org/pdf/2312.02003.pdf], we can better prepare and fortify our models against potential vulnerabilities, setting the stage for a more secure and reliable AI-driven future.\n",
    "\n",
    "Consider a hypothetical, yet highly plausible, scenario involving an advanced Large Language Model (LLM) named CPT, which now possesses the novel feature of sending emails. Users input the email's destination and context, but there's a vulnerability – a manipulation that enables the model to use another user's email address as the sender or to access and share the content of another person's email. While a straightforward solution might be to segregate the content from the email destination, ensuring that the latter is directly delivered to the function without being influenced by the user's contextual input or the LLM, this only mitigates one of potentially hundreds of threats posed by this new feature another example is \"Autonomously Hack Websites\" [https://arxiv.org/html/2402.06664v1].\n",
    "\n",
    "The real-world examples are numerous, spanning various domains such as:\n",
    "\n",
    "Computer vision: Well-documented are the adversarial attacks and universal attacks on computer vision, which pose significant security threats in areas like autonomous transportation. These attacks could, for instance, cause a self-driving car to miss a stop sign or a drone to crash into a building. They also pose risks to military autonomous systems, which can be manipulated by adversarial attacks. More information on these attacks can be found at [https://arxiv.org/abs/2108.00401].\n",
    "\n",
    "Recommendation systems: Recommendation systems, a market worth over one trillion dollars that relies on matching advertisements to users, are vulnerable to bot attacks. These attacks corrupt data by identifying and exploiting the model's sensitive perturbations, making manipulation achievable even with a small-scale bot attack. Further details are available at [https://arxiv.org/pdf/2110.13980.pdf] and [https://arxiv.org/pdf/2310.07159.pdf].\n",
    "\n",
    "LLMs: Large Language Models (LLMs) face specific types of attacks, such as prompt injection attacks, which can be found at [https://arxiv.org/abs/2108.00401]. These attacks can manipulate the model to generate false information, produce toxic content, or create content that could manipulate stock markets and trigger unwanted operations in external systems. Additional information on these attacks is available at [https://arxiv.org/html/2402.06664v1]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Behind the Scenes: Logic-Shift Ptompt Injunction Attacks Reinforcement Learning jailbreaks\n",
    "Let's delve deeper into the main concept of LLM attacks, akin to those seen in computer vision where misclassification can result from adversarial attacks [See: \"Adversarial Attacks in Computer Vision\" at https://arxiv.org/abs/2108.00401]. Similarly, LLMs can be manipulated to produce outputs that diverge from the user's intentions or the model owner's guidelines by perturbing the input, thereby shifting it to a different location in the \"policy space\" that the LLM has been trained and fine-tuned to adhere to. We believe that such manipulations cannot be fully prevented by merely adjusting the LLM's parameters and may even be achieved with relative ease and intuitiveness.\n",
    "Imagine we had a model that is fully secure within the task space of harmful output content, consistently providing answers generated without filter blocking, always adhering to the policy it developed during training. Now, consider that while LLM models are vast, not all aspects of this freedom translate directly into performance, and sometimes barely influence LLM models. For instance, in updating the knowledge base of an LLM, rather than updating all billion parameters, methods like retrieval and fine-tuning are employed [Reference: \"Knowledge Update for LLM\" at https://arxiv.org/pdf/2312.05934.pdf]. For perspective, in our scenario, our model has 10 billion well-trained parameters. Suppose our word vocabulary is N, the number of input tokens is S, and our original prompt consists of t words. Thus, the number of possible inputs is N^(S-t). To grasp the scale of potential injection attacks that add prompts to the input, consider if N=5000, S=1000, t=100. The number of potential prompt injection attacks that the model needs to be shielded from could be around 50^(1000-100) in the optimistic scenario of 50 valid possibilities for each token place of the injection attack. This illustrates why such attacks are challenging to prevent [See: \"Challenges in Preventing Injection Attacks\" at https://arxiv.org/pdf/2311.17391.pdf] and why traditional protections are not practical enough against a vast scale of injection attacks. These often lead to the blocking of output based on a toxicity score provided by another classification model after analyzing the output [FIGURE], which, while it limits LLM capabilities, does not provide a long-term solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intuative attacks examples for LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Injuction attacks\n",
    "An injection attack, as described in the paper \"Injuction Attacks on Language Models\" [https://arxiv.org/pdf/2306.05499.pdf], is a type of cyberattack targeting language models like those used in natural language processing. In this kind of attack, an adversary manipulates the input to a language model in a way that causes it to generate or process data in an unintended manner. This manipulation often involves inserting or altering text in such a way that the model's output deviates from its intended function or policy. The goal of these attacks can vary from causing the model to produce nonsensical or biased responses, to leaking sensitive information, or to executing actions that benefit the attacker. Injection attacks exploit the vulnerabilities in the way these models process and interpret input data, highlighting the importance of robust security measures in the design and deployment of language models. [https://arxiv.org/abs/2302.12173][FIG bing]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt attack that make the any input under the Task \"guide steps\" shiffted, working on GPT-3.5 and GPT-4,\n",
    "GPT-3.5 give us the original output\n",
    "GPT-4 we perhapse GPT-4 give us the block automatic response "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://arxiv.org/html/2402.09154v1\n",
    "Attacking Large Language Models with Projected Gradient Descent\n",
    "\n",
    "https://arxiv.org/pdf/2305.03495.pdf  Beam Search Attacks on Language Models\n",
    "\n",
    "\n",
    "The model:\n",
    "\n",
    "# LLM for code "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
